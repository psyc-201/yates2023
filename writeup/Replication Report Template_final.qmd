---
title: "Replication of Study Robust holistic face processing in early childhood
during the COVID-19 pandemic by Yates & Lewkowicz (2023, Journal of Experimental Child Psychology)"
author: "Replication Author[s]: Anna Pusok (annapusok@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

## Introduction

Face perception research examines how humans process and recognize faces, a critical skill for social interaction. One key measure of face processing is the composite face effect (CFE; Young et al., 1987), which demonstrates holistic processing: when two face halves are spatially aligned, people struggle to judge whether the top halves match because they automatically integrate both halves into a unified whole. This interference disappears when the halves are misaligned, revealing that faces are processed configurally rather than feature-by-feature.

Yates & Lewkowicz (2023) investigated when the CFE emerges developmentally by testing typically developing children aged 4-6 years. Using child-appropriate stimuli (faces with colored tints on top halves) and procedures (36 trials with unlimited viewing time), they found that children as young as 4 exhibit holistic face processing. I am replicating this study with neurotypical adults (n=25) to validate the paradigm before extending it to to children and adults with autism spectrum disorder (which is the broad framewrok of my research program). Key modifications include increasing trials from 36 to 96, using grayscale stimuli, implementing time constraints (1000ms face presentation, 3000ms response window), and adding the Social Responsiveness Scale, questionnaire that quantifies autistic traits and social impairment. I expect to observe a significant composite face effect in adults, with lower accuracy for aligned-same versus misaligned-same trials.

Link to paper: https://github.com/annapusok731/yates2023/tree/2885fd3f36660e431e4876c7b2f4dec89f9b7a79/original_paper

Repository: https://github.com/annapusok731/yates2023.git

OSF Preregistration: https://osf.io/a7edz/overview


## Methods

### Power Analysis

Power analysis indicated that a minimum sample size of 17 participants would achieve 80% power to detect the composite face effect. However, because my broader research program involves comparing neurotypical and ASD groups, I based my target sample size on Hsiung & Chien (2024), who conducted similar CFE comparisons between clinical and control groups. Following their methodology, which set α = 0.05 and achieved power of approximately 0.84, I determined that 24 participants per group would provide adequate power for future between-group analyses. For this initial validation study with neurotypical adults, I will recruit approximately 30 participants to account for potential exclusions and ensure a final sample of 24.

### Planned Sample

About 30 adults (# female gender,  other gender) participants (M age = X years, SD = Y, range = X-Y) will be recruited for the final project. The target final sample size is 24 adults, consistent with the power analysis described above (Hsiung & Chien, 2024). Demographic information including age, gender, and Social Responsiveness Scale scores will be collected and reported in the final results.

### Materials
Composite face stimuli will be created using Canva from the Chicago Face Database at (https://www.chicagofaces.org/). Faces will be grayscale images of White males and females with a neutral expression looking directly into the camera. 64 pairs of composite face stimuli (32 male and 32 female pairs) will be presented to each participant. "Given that misalignment of inner face features can reduce the composite face effect (Curby & Entenman, 2016; but see Kurbel et al., 2021, for robust results regardless of perceptual fit), we ensured as much as possible that each individual face was paired with a same-gender face of similar size/shape and skin tone. In addition, as recommended for the composite face task (Rossion & Retter, 2015), we included a small gap between the top and bottom halves of each composite face."

"For each identity pair, we created 8 different composite faces from combinations of the top and bottom halves of the faces (these can be seen in Fig. 1 A). Four of these composite faces consisted of spatially aligned top and bottom halves of faces, and the other four consisted of spatially misaligned top and bottom halves of faces (in the misaligned composite faces, the top half of the face was shifted ∼1.2 cm to the left of the bottom half of the face). As can be seen in Fig. 1A, Composite Face AA consisted of the top and bottom halves of Identity A, Composite Face AB consisted of the top half of Identity A and the bottom half of Identity B, Composite Face BA consisted of the top half of Identity B and the bottom half of Identity A, and Composite Face BB consisted of the top and bottom halves of Identity B. To minimize the impact of external face features, we removed all hair and ears from the original images by using Adobe Photoshop 2020 and added a slight purple–pink tint to the top halves of the faces to draw children’s attention to the top halves (de Heering et al., 2007)."

The final full stimulus set is available at a public GitHub link: https://github.com/psyc-201/yates2023/tree/main/stimuli/

### Procedure	

Following Institutional Review Board approval, participants were recruited through the university's Sona system and received course credits for participation. After enrolling through their Sona accounts, participants completed the experiment online.

The experiment began with two practice trials to familiarize participants with the task. In the first practice trial, participants viewed a pair of spatially aligned composite faces with different top halves presented side-by-side on the screen. They were instructed to judge whether the top portions of the two faces were the same or different by clicking the appropriate response button. Feedback was provided: incorrect responses prompted "Try Again," while correct responses were confirmed with "Great job! The top parts of the face are the same." The second practice trial presented spatially misaligned composite faces with identical top halves, and participants again judged whether the top portions were the same or different, receiving the same feedback structure. Participants were required to respond correctly during the practice trials, before advancing to the test phase.

The test phase consisted of 96 trials presenting four trial types: (a) aligned-same, where the top halves depicted the same identity and were horizontally aligned with the bottom halves; (b) aligned-different, where the top halves depicted different identities and were horizontally aligned; (c) misaligned-same, where the top halves depicted the same identity but were horizontally offset from the bottom halves; and (d) misaligned-different, where the top halves depicted different identities and were horizontally offset. Similarly to Yates and Lewkowicz, the trials during the test phase only move forward, once participants selected an answer by pressing on either the "Same" or "Different" buttons that are present on the screen with the face pairs.

After completing the CFE task, participants completed a brief demographic survey assessing age, race/ethnicity, education level, and residential setting (urban, suburban, or rural). Participants also completed the Social Responsiveness Scale-2 Adult form (SRS-2), a standardized questionnaire measuring social communication abilities and autism-related traits.

### Analysis Plan

"Response accuracy
The data of primary interest were the accuracy scores obtained in the same trials. Specifically, lower accuracy scores in the aligned–same trials than in the misaligned–same trials is generally considered to reflect holistic processing. The data of secondary interest were the accuracy scores obtained in the different trials. In this case, the accuracy scores in the aligned–different trials and in the misaligned–different trials should be the same. These different trial scores indicate how well children were able to detect differences when the top halves of the composite faces actually differed and, thus, provide a baseline against which to evaluate the accuracy data from the same trials. To statistically assess response accuracy, we performed mixed repeated-measures analysis of variance (ANOVA) models with alignment as a within-participants factor, age and gender as between-participants factors, and participant as a random effect. For all analyses, we included gender as a predictor given prior research showing that there may be differences in holistic face processing even in early childhood (Stajduhar et al., 2022)."

### Differences from Original Study

While maintaining the core experimental design of Yates & Lewkowicz (2023), I made several methodological changes to optimize the paradigm for adult participants and align with my research goals.

I increased the number of trials from 36 to 96 to enhance statistical power while remaining appropriate for adult attention spans. To accommodate this increase, I expanded the stimulus set from 24 to 64 unique face pairs, ensuring that participants encountered novel stimuli throughout most of the experiment. Unlike the original study, which allowed unlimited viewing time for participants ages 4-6, I presented each face pair for 1 second with a 3-second response window to maintain task engagement and collect reaction time data. Additionally, whereas the original study applied a purple tint to the top face halves to direct children's attention, I used grayscale stimuli, as this attentional cue was unnecessary for adult participants.

Beyond accuracy (the sole dependent variable in the original study), I recorded reaction times to enable more nuanced analyses of holistic processing efficiency. I also added the Social Responsiveness Scale-2 Adult form (SRS-2) and a demographic questionnaire to characterize the sample's social communication profiles. I omitted the COVID-19 mask exposure questionnaire from the original study because Yates & Lewkowicz (2023) specifically examined whether pandemic-related mask exposure influenced face processing development in young children. This question is not relevant to my research focus, which centers on comparing holistic face processing between neurotypical and autistic individuals. Additionally, the orginial study added age in the ANOVA as they comparted the composite face effect in 4- 5- and 6-year olds, however, I did not include age as myb sample was an adult sample.

My exclusion criteria focused solely on task performance (responding at or below chance level: <50%), whereas the original study excluded participants based on whether participants completed the full experiment or not and also if participants had ASD diagnosis. For data analysis, I did not stratify by age groups as the original study did (comparing 4-, 5-, and 6-year-olds separately), since all participants were adults and developmental trajectory was not a focus of this validation study.

### Methods Addendum (Post Data Collection)

#### Actual Sample

I deviated from the OSF preregistration in final sample size: I originally planned to collect data from 24 adults but ultimately recruited 26 participants. One participant was excluded for performing below chance level (<50% accuracy), resulting in a final analyzable sample of 25 adults (3 Male, 1 Other; M age = 20.44 years, SD = 2.64, range: 18-30). The final sample included 9 Asian American, 7 Hispanic and Latino, 6 White, 2 Mixed Race and 1 Other race individuals.

#### Differences from pre-data collection methods plan
  
I made two significant methodological changes during the piloting phase.
Initially, I applied a colored tint to the top halves of the faces, following Yates & Lewkowicz's approach for directing children's attention. However, pilot testing revealed this attentional cue was unnecessary for adults, so I modified the stimuli to be fully grayscale for the final experiment.
I also changed the trial timing. Unlike the original study, which allowed child participants unlimited viewing and response time, I implemented time constraints: Each trial began with a fixation cross presented for 1200ms, followed by the two composite faces displayed for 1000ms. Participants then saw the prompt "Are the top parts of the faces the same or different?" with two response buttons labeled "Same" and "Different." Participants had 3000ms to respond before the trial automatically advanced.

## Results

### Data preparation

One participant who performed at or below chance (below 50% accuracy) was excluded. 

Some of the codes have deviated from the OSF preregistration below. While the core analytical approach adheres to the preregistration, some coding procedures were adapted during implementation. All primary hypotheses and statistical tests remain as originally specified. 

Deviations to the code on OSF included: adding exclusion script to R Markdown, reorganizing bars on graphs, statistics added to graphs, new colors added to graphs. 

The final full analyses codes is available at a  public GitHub link: https://github.com/psyc-201/yates2023/tree/main/codes/

The following codes are in the repo:
- stats_final_analyses.rmd -- analysis code that was linked for OSF preregistration
- stats_final_analyses_updated.rmd -- final analyses code that deviated from the preregistered analyses in some details (specified below)
- srs.rmd for reorganizing and calculating SRS scores for each participant
- demographic_info_stats_finalproject.rmd for extracting demographic information from individual files

```{r include=F, error=TRUE}
# ### Data Preparation #Remove comments
# 
# #### Load Relevant Libraries and Functions
# 
# knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# 
# # Load required libraries
# # library(tidyverse)
# # library(lme4)
# # library(lmerTest)
# # library(emmeans)
# # library(ez)
# # library(effsize)
# # library(readxl)
# 
# #### Import data
# 
# # data_path <- "/Users/annapusok/Desktop/stats_final_data/"
# 
# # List CSV files from that location
# file_list <- list.files(path = data_path, pattern = "*.csv", full.names = TRUE)
# 
# print(paste("Current directory:", getwd()))
# print(paste("Found", length(file_list), "CSV files"))
# print("Files found:")
# print(file_list)
# 
# # Check where you are
# print(paste("Current directory:", getwd()))
# 
# # List CSV files - UPDATE THIS PATH to where your CSV files are
# file_list <- list.files(pattern = "*.csv", full.names = TRUE)
# 
# # Debug: print files found
# print(paste("Found", length(file_list), "CSV files"))
# 
# 
# # Debug: print files found
# print(paste("Found", length(file_list), "CSV files"))
# 
# all_data <- map_df(file_list, ~{
#   print(paste("Reading:", .x))  # See which file is being read
#   
#   data <- read_csv(.x, col_types = cols(
#     response_time_ms = col_double(),
#     correct = col_character(),
#     .default = col_guess()
#   ))
#   
#   # Check if correct column exists in this file
#   print(paste("Correct column exists in", .x, ":", "correct" %in% names(data)))
#   
#   # Fill down sex column within each file
#   if("sex" %in% names(data)) {
#     data <- data %>% fill(sex, .direction = "down")
#   }
#   return(data)
# })
# 
# #### Data exclusion / filtering
# 
# # Convert correct to logical
# #all_data <- all_data %>%
#   #mutate(correct = as.logical(correct))
# 
# # Filter main task trials only
# 
# main_data <- all_data %>%
#   filter(task == "main", !is.na(correct), !is.na(response_time_ms))
# 
# # Create alignment and correct_response variables
# main_data <- main_data %>%
#   mutate(
#     alignment = case_when(
#       condition %in% c("diff_align", "same_aligned") ~ "aligned",
#       condition %in% c("diff_misalign", "same_misaligned") ~ "misaligned"
#     ),
#     correct_response = case_when(
#       condition %in% c("diff_align", "diff_misalign") ~ "different",
#       condition %in% c("same_aligned", "same_misaligned") ~ "same"
#     ),
#     alignment = factor(alignment, levels = c("misaligned", "aligned"))
#   )
# 
# # Load demographics and merge
# demographics <- read_xlsx("/Users/annapusok/Desktop/stats_final_data/participant_demographics.xlsx")
# main_data <- main_data %>%
#   select(-sex) %>%
#   left_join(demographics %>% select(subject_id, sex, srs_total_score), by = "subject_id")
# 
# cat("\nMain trials after filtering:", nrow(main_data), "\n")
# 
# #### Prepare data for analysis - create columns etc.
# 
# # Report trials with no response
# total_main_trials <- sum(all_data$task == "main", na.rm = TRUE)
# trials_with_response <- nrow(main_data)
# trials_no_response <- total_main_trials - trials_with_response
# 
# cat("\nTrials excluded due to no response:", trials_no_response, 
#     "out of", total_main_trials, 
#     sprintf("(%.1f%%)", 100 * trials_no_response / total_main_trials), "\n")
# 
# # Calculate accuracy by participant
# participant_accuracy <- main_data %>%
#   group_by(subject_id) %>%
#   summarise(
#     n_trials = n(),
#     n_correct = sum(correct),
#     accuracy = mean(correct),
#     .groups = "drop"
#   )
# 
# # Identify participants performing below chance
# below_chance <- participant_accuracy %>%
#   filter(accuracy < 0.5)
# 
# cat("\nTotal participants:", nrow(participant_accuracy), "\n")
# cat("Participants performing below chance (<50%):", nrow(below_chance), "\n\n")
# 
# if (nrow(below_chance) > 0) {
#   cat("Participants with below-chance performance:\n")
#   print(below_chance, n = Inf)
#   
#   cat("\n--- Binomial Tests (testing if significantly below chance) ---\n")
#   for (i in 1:nrow(below_chance)) {
#     p_id <- below_chance$subject_id[i]
#     n_correct <- below_chance$n_correct[i]
#     n_trials <- below_chance$n_trials[i]
#     
#     binom_test <- binom.test(n_correct, n_trials, p = 0.5, alternative = "less")
#     
#     cat(sprintf("Participant %s: %d/%d correct (%.1f%%), p = %.4f %s\n",
#                 p_id, n_correct, n_trials, below_chance$accuracy[i] * 100,
#                 binom_test$p.value,
#                 ifelse(binom_test$p.value < 0.05, "**", "")))
#   }
# }
```

### Confirmatory analysis

Accuracy & RT
For accuracy data, we will conduct a mixed repeated-measures ANOVA with alignment as a within-participant factor and gender as a between-participant factor, with participant included as a random effect. This analysis will allow us to test for main effects of alignment and gender, as well as their interaction. For reaction time data, we will conduct a parallel mixed repeated-measures ANOVA with the same factors to examine whether alignment affects processing speed. If we observe significant effects of gender on either accuracy or reaction time, we will follow up with pairwise comparisons across gender groups. These comparisons will be conducted using two-tailed t-tests with Bonferroni correction to control for multiple comparisons. These planned analyses will allow us to determine whether holistic face processing, as indexed by both accuracy and response speed, differs by gender and whether such differences are specific to alignment effects or reflect broader group differences in face processing. 


### Confirmatory Results

Adults demonstrated a significant composite face effect. Accuracy was significantly lower for aligned-same trials compared to misaligned-same trials F(1, 24) = 21.27, p < .001, ηp² = .47. This replicates the core finding F(1, 127) = 70.57, p < .001, η G 2 = .104 from Yates & Lewkowicz (2023).


### Exploratory analyses

For both accuracy and reaction time data, I explored potential relationships between social responsiveness and holistic face processing. First, I examined whether SRS total scores correlate with overall task performance by conducting Pearson correlations between SRS scores and (a) average accuracy across all trials and (b) average reaction time across all trials. More critically, to test whether individual differences in social responsiveness relate specifically to holistic processing, I examined correlations between SRS scores and the composite face effect magnitude, calculated as the difference in accuracy and reaction time between aligned-same and misaligned-same trials. 

### Analyses

#Final analyses code in GitHub Repo: stats_final_analyses_updated.rmd

```{r echo=TRUE, eval=FALSE}

# #Accuracy Analyses
# 
# # Calculate accuracy by subject and condition
# accuracy_by_subject <- main_data %>%
#   group_by(subject_id, alignment, correct_response) %>%
#   summarise(
#     accuracy = mean(correct, na.rm = TRUE),
#     n_trials = n(),
#     .groups = "drop"
#   )
# 
# # Overall means and SEs
# accuracy_means <- accuracy_by_subject %>%
#   group_by(alignment, correct_response) %>%
#   summarise(
#     mean_accuracy = mean(accuracy),
#     se_accuracy = sd(accuracy) / sqrt(n()),
#     n_subjects = n(),
#     .groups = "drop"
#   )
# 
# print(accuracy_means)
# 
# # Paired t-tests for accuracy
# cat("\n--- Accuracy: Different Trials ---\n")
# diff_accuracy <- accuracy_by_subject %>%
#   filter(correct_response == "different") %>%
#   pivot_wider(names_from = alignment, values_from = accuracy, id_cols = subject_id)
# 
# if(all(c("aligned", "misaligned") %in% names(diff_accuracy))) {
#   diff_acc_test <- t.test(diff_accuracy$aligned, diff_accuracy$misaligned, paired = TRUE)
#   print(diff_acc_test)
#   cat("Effect size (Cohen's d):", 
#       (mean(diff_accuracy$aligned) - mean(diff_accuracy$misaligned)) / 
#         sd(diff_accuracy$aligned - diff_accuracy$misaligned), "\n")
# }
# 
# cat("\n--- Accuracy: Same Trials ---\n")
# same_accuracy <- accuracy_by_subject %>%
#   filter(correct_response == "same") %>%
#   pivot_wider(names_from = alignment, values_from = accuracy, id_cols = subject_id)
# 
# if(all(c("aligned", "misaligned") %in% names(same_accuracy))) {
#   same_acc_test <- t.test(same_accuracy$aligned, same_accuracy$misaligned, paired = TRUE)
#   print(same_acc_test)
#   cat("Effect size (Cohen's d):", 
#       (mean(same_accuracy$aligned) - mean(same_accuracy$misaligned)) / 
#         sd(same_accuracy$aligned - same_accuracy$misaligned), "\n")
# }
# 
# # Additional analysis for same trials with effect size
# same_cohen_d <- cohen.d(same_accuracy$aligned, same_accuracy$misaligned, paired = TRUE)
# cat("\n=== Aligned Same vs Misaligned Same ===\n")
# cat("Aligned Same Mean:", mean(same_accuracy$aligned) * 100, "%\n")
# cat("Misaligned Same Mean:", mean(same_accuracy$misaligned) * 100, "%\n")
# cat("Cohen's d:", same_cohen_d$estimate, "\n")
# 
# # Repeated-measures ANOVA for Same trials
# same_trials_agg <- main_data %>%
#   filter(correct_response == "same") %>%
#   group_by(subject_id, alignment) %>%
#   summarise(mean_accuracy = mean(correct, na.rm = TRUE), .groups = "drop") %>%
#   mutate(
#     subject_id = factor(subject_id),
#     alignment = factor(alignment)
#   )
# 
# ez_result <- ezANOVA(
#   data = same_trials_agg,
#   dv = mean_accuracy,
#   wid = subject_id,
#   within = alignment,
#   detailed = TRUE,
#   type = 3
# )
# print(ez_result)
# 
# # Mixed ANOVA with sex as between-subjects factor
# cat("\n--- Mixed ANOVA: Accuracy on Same Trials (with sex) ---\n")
# same_trials <- main_data %>% filter(correct_response == "same")
# accuracy_model_same <- lmer(correct ~ alignment * sex + (1|subject_id), 
#                             data = same_trials)
# print(summary(accuracy_model_same))
# print(anova(accuracy_model_same))
# ```
# 
# ```{r accuracy-plot, fig.width=8, fig.height=6}, r include=F, error=TRUE
# # Prepare data for plotting
# accuracy_same <- accuracy_means %>% 
#   filter(correct_response == "same") %>%
#   mutate(alignment = factor(alignment, levels = c("aligned", "misaligned")))
# 
# # Calculate significance annotation position
# max_y <- max(accuracy_same$mean_accuracy) * 100 + max(accuracy_same$se_accuracy) * 100
# sig_y <- max_y + 3
# 
# sig_annotation <- data.frame(
#   x = 1.5,
#   y = sig_y,
#   label = "***"
# )
# 
# # Create plot
# accuracy_plot <- ggplot(accuracy_same, 
#                         aes(x = alignment, y = mean_accuracy * 100, fill = alignment)) +
#   geom_bar(stat = "identity", width = 0.4) +
#   geom_errorbar(aes(ymin = (mean_accuracy - se_accuracy) * 100, 
#                     ymax = (mean_accuracy + se_accuracy) * 100),
#                 width = 0.2) +
#   geom_hline(yintercept = 50, linetype = "dashed", color = "black", linewidth = 0.5) +
#   # Significance bracket
#   geom_segment(aes(x = 1, xend = 2, y = sig_y, yend = sig_y),
#                inherit.aes = FALSE, linewidth = 0.5) +
#   geom_segment(aes(x = 1, xend = 1, y = sig_y - 1, yend = sig_y),
#                inherit.aes = FALSE, linewidth = 0.5) +
#   geom_segment(aes(x = 2, xend = 2, y = sig_y - 1, yend = sig_y),
#                inherit.aes = FALSE, linewidth = 0.5) +
#   geom_text(data = sig_annotation, aes(x = x, y = y + 1.5, label = label),
#             inherit.aes = FALSE, size = 5, fontface = "bold") +
#   scale_fill_manual(values = c("aligned" = "#8482e6", "misaligned" = "#98FB98"),
#                     labels = c("Aligned Same", "Misaligned Same")) +
#   labs(title = "Accuracy for Same Trials (Aligned vs. Misaligned)",
#        y = "Accuracy (% Correct)", 
#        x = "Alignment Condition",
#        fill = "Alignment",
#        caption = "*** p < 0.001") +
#   theme_minimal(base_size = 14) +
#   theme(legend.position = "top",
#         panel.grid.major.x = element_blank(),
#         plot.caption = element_text(hjust = 0, size = 10)) +
#   scale_x_discrete(labels = c("Aligned", "Misaligned")) +
#   ylim(0, 105)
# 
# print(accuracy_plot)
# ggsave("accuracy_same_plot.png", width = 8, height = 6, dpi = 300)
# 
# 
# 
# 
# # RT Analyses
# 
# ```{r echo=TRUE, eval=FALSE}
# # Reaction Time Analysis
# 
# # Filter for correct trials only
# correct_data <- main_data %>%
#   filter(correct == 1)
# 
# # Calculate RT by subject and condition
# rt_by_subject <- correct_data %>%
#   group_by(subject_id, alignment, correct_response) %>%
#   summarise(
#     mean_rt = mean(response_time_ms, na.rm = TRUE),
#     n_trials = n(),
#     .groups = "drop"
#   )
# 
# # Overall RT means and SEs
# rt_means <- rt_by_subject %>%
#   group_by(alignment, correct_response) %>%
#   summarise(
#     mean_rt = mean(mean_rt),
#     se_rt = sd(mean_rt) / sqrt(n()),
#     n_subjects = n(),
#     .groups = "drop"
#   )
# 
# print(rt_means)
# ```
# 
# 
# ```{r echo=TRUE, eval=FALSE}
# # Paired t-tests for RT
# cat("\n--- RT: Different Trials ---\n")
# diff_rt <- rt_by_subject %>%
#   filter(correct_response == "different") %>%
#   pivot_wider(names_from = alignment, values_from = mean_rt, id_cols = subject_id)
# 
# diff_rt_pval <- NA
# if(all(c("aligned", "misaligned") %in% names(diff_rt))) {
#   diff_rt_test <- t.test(diff_rt$aligned, diff_rt$misaligned, paired = TRUE)
#   print(diff_rt_test)
#   diff_rt_pval <- diff_rt_test$p.value
#   cat("Effect size (Cohen's d):", 
#       (mean(diff_rt$aligned) - mean(diff_rt$misaligned)) / 
#         sd(diff_rt$aligned - diff_rt$misaligned), "\n")
# }
# 
# cat("\n--- RT: Same Trials ---\n")
# same_rt <- rt_by_subject %>%
#   filter(correct_response == "same") %>%
#   pivot_wider(names_from = alignment, values_from = mean_rt, id_cols = subject_id)
# 
# same_rt_pval <- NA
# if(all(c("aligned", "misaligned") %in% names(same_rt))) {
#   same_rt_test <- t.test(same_rt$aligned, same_rt$misaligned, paired = TRUE)
#   print(same_rt_test)
#   same_rt_pval <- same_rt_test$p.value
#   cat("Effect size (Cohen's d):", 
#       (mean(same_rt$aligned) - mean(same_rt$misaligned)) / 
#         sd(same_rt$aligned - same_rt$misaligned), "\n")
# }
# ``{r echo=TRUE, eval=FALSE}
# # Repeated-measures ANOVA for Different trials
# diff_trials_rt_agg <- correct_data %>%
#   filter(correct_response == "different") %>%
#   group_by(subject_id, alignment) %>%
#   summarise(mean_rt = mean(response_time_ms, na.rm = TRUE), .groups = "drop") %>%
#   mutate(subject_id = factor(subject_id), alignment = factor(alignment))
# 
# cat("\n--- Repeated-Measures ANOVA: RT on Different Trials ---\n")
# ez_diff_rt <- ezANOVA(
#   data = diff_trials_rt_agg,
#   dv = mean_rt,
#   wid = subject_id,
#   within = alignment,
#   detailed = TRUE,
#   type = 3
# )
# print(ez_diff_rt)
# 
# # Repeated-measures ANOVA for Same trials
# same_trials_rt_agg <- correct_data %>%
#   filter(correct_response == "same") %>%
#   group_by(subject_id, alignment) %>%
#   summarise(mean_rt = mean(response_time_ms, na.rm = TRUE), .groups = "drop") %>%
#   mutate(subject_id = factor(subject_id), alignment = factor(alignment))
# 
# cat("\n--- Repeated-Measures ANOVA: RT on Same Trials ---\n")
# ez_same_rt <- ezANOVA(
#   data = same_trials_rt_agg,
#   dv = mean_rt,
#   wid = subject_id,
#   within = alignment,
#   detailed = TRUE,
#   type = 3
# )
# print(ez_same_rt)
# 
# # Extract p-values for plotting
# diff_rt_pval <- ez_diff_rt$ANOVA$p[2]
# same_rt_pval <- ez_same_rt$ANOVA$p[2]
# 
# # Report in APA format
# cat("\n--- APA Format Results ---\n")
# cat("Different trials RT: F(1, 24) =", round(ez_diff_rt$ANOVA$F[2], 2), 
#     ", p =", round(diff_rt_pval, 4), 
#     ", ηG² =", round(ez_diff_rt$ANOVA$ges[2], 3), "\n")
# cat("Same trials RT: F(1, 24) =", round(ez_same_rt$ANOVA$F[2], 2), 
#     ", p =", round(same_rt_pval, 4), 
#     ", ηG² =", round(ez_same_rt$ANOVA$ges[2], 3), "\n")
# ```
# 
# ```{r echo=TRUE, eval=FALSE}
# # Prepare data for plotting
# rt_means <- rt_means %>%
#   mutate(
#     alignment = factor(alignment, levels = c("aligned", "misaligned")),
#     correct_response = factor(correct_response, levels = c("same", "different")),
#     combined = interaction(alignment, correct_response, sep = "_"),
#     combined = factor(combined, levels = c(
#       "aligned_same", "misaligned_same",
#       "aligned_different", "misaligned_different"
#     ))
#   )
# 
# # Define colors
# combined_colors <- c(
#   "aligned_same" = "#8482e6",
#   "misaligned_same" = "#98FB98",
#   "aligned_different" = "#524F81",
#   "misaligned_different" = "#228B22"
# )
# 
# # Create plot
# rt_plot <- ggplot(rt_means, aes(x = combined, y = mean_rt, fill = combined)) +
#   geom_bar(stat = "identity", width = 0.6) +
#   geom_errorbar(aes(ymin = mean_rt - se_rt, ymax = mean_rt + se_rt), width = 0.2) +
#   # Significance brackets for same trials
#   geom_segment(aes(x = 1, xend = 2, y = 740, yend = 740), 
#                color = "black", linewidth = 0.5) +
#   geom_segment(aes(x = 1, xend = 1, y = 740, yend = 730), 
#                color = "black", linewidth = 0.5) +
#   geom_segment(aes(x = 2, xend = 2, y = 740, yend = 730), 
#                color = "black", linewidth = 0.5) +
#   annotate("text", x = 1.5, y = 755, label = "***", size = 5) +
#   # Significance brackets for different trials
#   geom_segment(aes(x = 3, xend = 4, y = 740, yend = 740), 
#                color = "black", linewidth = 0.5) +
#   geom_segment(aes(x = 3, xend = 3, y = 740, yend = 730), 
#                color = "black", linewidth = 0.5) +
#   geom_segment(aes(x = 4, xend = 4, y = 740, yend = 730), 
#                color = "black", linewidth = 0.5) +
#   annotate("text", x = 3.5, y = 755, label = "ns", size = 5) +
#   scale_fill_manual(values = combined_colors,
#                     labels = c("Aligned\nSame", "Misaligned\nSame",
#                               "Aligned\nDifferent", "Misaligned\nDifferent")) +
#   labs(title = "Reaction Time by Alignment and Response Type",
#        y = "Reaction Time (ms)", 
#        x = "Condition",
#        fill = "Trial Type",
#        caption = "*** p < 0.001, ns = not significant") +
#   theme_minimal(base_size = 14) +
#   theme(legend.position = "top",
#         panel.grid.major.x = element_blank(),
#         plot.caption = element_text(hjust = 0, size = 10)) +
#   coord_cartesian(ylim = c(0, 800))
# 
# print(rt_plot)
# ggsave("rt_plot.png", width = 8, height = 6, dpi = 300)
# 
# # Individual participant trajectories
# ggplot(rt_by_subject, aes(x = alignment, y = mean_rt, col = correct_response)) +
#   geom_point() +
#   geom_line(aes(group = subject_id)) +
#   facet_wrap(~correct_response) +
#   scale_color_manual(values = c("#38b6ff", "#ff914d")) +
#   labs(title = "Individual RT Trajectories by Alignment",
#        y = "Mean RT (ms)",
#        x = "Alignment") +
#   theme_minimal()
# 
# ggsave("rt_by_subject.png", width = 8, height = 6, dpi = 300)
# ```
# 
# # Exploratory Analyses: SRS Correlations
# 
# ```{r include=F, error=TRUE}
# # Calculate per-participant performance metrics
# participant_performance <- main_data %>%
#   group_by(subject_id) %>%
#   summarise(
#     mean_accuracy = mean(correct, na.rm = TRUE),
#     mean_rt = mean(response_time_ms, na.rm = TRUE),
#     n_trials = n(),
#     .groups = "drop"
#   )
# 
# # Merge with SRS scores
# analysis_data <- participant_performance %>%
#   left_join(demographics %>% select(subject_id, srs_total_score), by = "subject_id") %>%
#   filter(!is.na(srs_total_score))
# 
# cat("\n=== Data Summary ===\n")
# cat("Participants with SRS scores:", nrow(analysis_data), "\n")
# 
# # Pearson correlations
# cat("\n=== Correlation: SRS Total Score and Average Accuracy ===\n")
# cor_accuracy <- cor.test(analysis_data$srs_total_score, 
#                          analysis_data$mean_accuracy, 
#                          method = "pearson")
# print(cor_accuracy)
# 
# cat("\n=== Correlation: SRS Total Score and Average Reaction Time ===\n")
# cor_rt <- cor.test(analysis_data$srs_total_score, 
#                    analysis_data$mean_rt, 
#                    method = "pearson")
# print(cor_rt)

# # Calculate CFE magnitude per participant
# #cfe_magnitude <- main_data %>%
#   filter(correct_response == "same") %>%
#   group_by(subject_id, alignment) %>%
#   summarise(accuracy = mean(correct), .groups = "drop") %>%
#   pivot_wider(names_from = alignment, values_from = accuracy) %>%
#   mutate(cfe_magnitude = misaligned - aligned)  # Higher = stronger CFE
# 
# # Correlate SRS with CFE magnitude
# cor.test(cfe_magnitude$srs_total_score, cfe_magnitude$cfe_magnitude)
# 
# 
# # # Summary table
# correlation_summary <- tibble(
#   Variable = c("Average Accuracy", "Average RT"),
#   r = c(cor_accuracy$estimate, cor_rt$estimate),
#   p_value = c(cor_accuracy$p.value, cor_rt$p.value),
#   CI_lower = c(cor_accuracy$conf.int[1], cor_rt$conf.int[1]),
#   CI_upper = c(cor_accuracy$conf.int[2], cor_rt$conf.int[2])
# )
# print(correlation_summary)
# ```
# 
# ```{r echo=TRUE, eval=FALSE}
# library(patchwork)
# 
# # Scatterplot: SRS vs Accuracy
# plot_accuracy <- ggplot(analysis_data, aes(x = srs_total_score, y = mean_accuracy)) +
#   geom_point(size = 3, alpha = 0.7, color = "#2E86AB") +
#   geom_smooth(method = "lm", se = TRUE, color = "#A23B72", fill = "#A23B72", alpha = 0.2) +
#   labs(title = "SRS Total Score vs. Average Accuracy",
#        x = "SRS Total Score",
#        y = "Average Accuracy") +
#   annotate("text", x = Inf, y = -Inf, 
#            label = paste0("r = ", round(cor_accuracy$estimate, 3), 
#                          "\np = ", round(cor_accuracy$p.value, 3)),
#            hjust = 1.1, vjust = -0.5, size = 4) +
#   theme_minimal() +
#   theme(plot.title = element_text(face = "bold", size = 14),
#         axis.title = element_text(size = 12))
# 
# # Scatterplot: SRS vs RT
# plot_rt <- ggplot(analysis_data, aes(x = srs_total_score, y = mean_rt)) +
#   geom_point(size = 3, alpha = 0.7, color = "#2E86AB") +
#   geom_smooth(method = "lm", se = TRUE, color = "#A23B72", fill = "#A23B72", alpha = 0.2) +
#   labs(title = "SRS Total Score vs. Average RT",
#        x = "SRS Total Score",
#        y = "Average RT (ms)") +
#   annotate("text", x = Inf, y = -Inf, 
#            label = paste0("r = ", round(cor_rt$estimate, 3), 
#                          "\np = ", round(cor_rt$p.value, 3)),
#            hjust = 1.1, vjust = -0.5, size = 4) +
#   theme_minimal() +
#   theme(plot.title = element_text(face = "bold", size = 14),
#         axis.title = element_text(size = 12))
# 
# # Combine plots
# combined_plot <- plot_accuracy | plot_rt
# print(combined_plot)
# 
# ggsave("SRS_correlations.png", combined_plot, width = 12, height = 5, dpi = 300)
# ```
# 
# # Export Summary Data
# 
# ```{r include=F, error=TRUE}
# write_csv(accuracy_means, "accuracy_summary.csv")
# write_csv(rt_means, "rt_summary.csv")
# write_csv(accuracy_by_subject, "accuracy_by_subject.csv")
# write_csv(rt_by_subject, "rt_by_subject.csv")
# write_csv(correlation_summary, "srs_correlation_summary.csv")
# 
# cat("\nAll summary files exported successfully!\n")
```
```

## Graphs

#Graphs for Accuracy Scores

![](https://raw.githubusercontent.com/psyc-201/yates2023/main/original_accuracy_same.png)

![](https://raw.githubusercontent.com/psyc-201/yates2023/main/replica_accuracy_same.png)

![](https://raw.githubusercontent.com/psyc-201/yates2023/main/replica_accuracy_all.png)

#Graphs for Reaction Time

![](https://raw.githubusercontent.com/psyc-201/yates2023/main/replica_rt.png)

#Exploratory Analyses

![](https://raw.githubusercontent.com/psyc-201/yates2023/main/replica_exploratory.png)


## Discussion

### Summary of Replication Attempt

The main result of Yates & Lewkowicz (2023) experiment 1 successfully replicated. Similarly to the original paper and others, I found evidence that adults process fces holistically, measured by the composite face effect.

However, I do acknowledge the methodological changes implemented in the study, that was due to using an adult sample instead of children ages 4-6.  

### Commentary

As mentioned above, I could replicate the main accuracy related findings, however, lot's of changes have been implemented to make the task more appropriate for adults.
